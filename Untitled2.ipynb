{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOCNlZXvHX9b4dQYDQ/rUZU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fw3HCWQ_ywIs","executionInfo":{"status":"ok","timestamp":1765228965271,"user_tz":-300,"elapsed":67337,"user":{"displayName":"Kishwer Basri","userId":"16853879575023419513"}},"outputId":"7253aa2b-53e7-4313-b3f6-c692e6821a5d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Epoch 1/50 - Loss: 0.1938\n","Epoch 2/50 - Loss: 0.1598\n","Epoch 3/50 - Loss: 0.1578\n","Epoch 4/50 - Loss: 0.1575\n","Epoch 5/50 - Loss: 0.1552\n","Epoch 6/50 - Loss: 0.1574\n","Epoch 7/50 - Loss: 0.1552\n","Epoch 8/50 - Loss: 0.1538\n","Epoch 9/50 - Loss: 0.1563\n","Epoch 10/50 - Loss: 0.1529\n","Epoch 11/50 - Loss: 0.1531\n","Epoch 12/50 - Loss: 0.1528\n","Epoch 13/50 - Loss: 0.1524\n","Epoch 14/50 - Loss: 0.1514\n","Epoch 15/50 - Loss: 0.1537\n","Epoch 16/50 - Loss: 0.1518\n","Epoch 17/50 - Loss: 0.1521\n","Epoch 18/50 - Loss: 0.1512\n","Epoch 19/50 - Loss: 0.1515\n","Epoch 20/50 - Loss: 0.1538\n","Epoch 21/50 - Loss: 0.1539\n","Epoch 22/50 - Loss: 0.1539\n","Epoch 23/50 - Loss: 0.1508\n","Epoch 24/50 - Loss: 0.1629\n","Epoch 25/50 - Loss: 0.1512\n","Epoch 26/50 - Loss: 0.1545\n","Epoch 27/50 - Loss: 0.1571\n","Epoch 28/50 - Loss: 0.1519\n","Epoch 29/50 - Loss: 0.1512\n","Epoch 30/50 - Loss: 0.1546\n","Epoch 31/50 - Loss: 0.1505\n","Epoch 32/50 - Loss: 0.1514\n","Epoch 33/50 - Loss: 0.1504\n","Epoch 34/50 - Loss: 0.1504\n","Epoch 35/50 - Loss: 0.1501\n","Epoch 36/50 - Loss: 0.1497\n","Epoch 37/50 - Loss: 0.1487\n","Epoch 38/50 - Loss: 0.1482\n","Epoch 39/50 - Loss: 0.1491\n","Epoch 40/50 - Loss: 0.1479\n","Epoch 41/50 - Loss: 0.1506\n","Epoch 42/50 - Loss: 0.1484\n","Epoch 43/50 - Loss: 0.1480\n","Epoch 44/50 - Loss: 0.1470\n","Epoch 45/50 - Loss: 0.1461\n","Epoch 46/50 - Loss: 0.1467\n","Epoch 47/50 - Loss: 0.1465\n","Epoch 48/50 - Loss: 0.1449\n","Epoch 49/50 - Loss: 0.1473\n","Epoch 50/50 - Loss: 0.1570\n","\n","Model Saved Successfully as fusion_model.pth!\n"]}],"source":["import os\n","import numpy as np\n","import rasterio\n","import cv2\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","# -----------------------------------------------------\n","# 1. DIRECTORIES (change if your folders are different)\n","# -----------------------------------------------------\n","MODIS_DIR = \"./modis/\"\n","S2_DIR = \"./s2/\"\n","IMAGE_COUNT = 20   # we are using 20 pairs\n","\n","# -----------------------------------------------------\n","# 2. DATASET CLASS\n","# -----------------------------------------------------\n","class FusionDataset(Dataset):\n","    def __init__(self, modis_dir, s2_dir, count=20):\n","        self.modis_dir = modis_dir\n","        self.s2_dir = s2_dir\n","        self.count = count\n","\n","    def __len__(self):\n","        return self.count\n","\n","    def __getitem__(self, idx):\n","        modis_path = os.path.join(self.modis_dir, f\"MODIS_sample_{idx}.tif\")\n","        s2_path = os.path.join(self.s2_dir, f\"S2_sample_{idx}.tif\")\n","\n","        # -------------------------\n","        # Load MODIS (2 bands)\n","        # -------------------------\n","        with rasterio.open(modis_path) as src:\n","            modis_raw = src.read([1, 2])    # b01=RED, b02=NIR\n","\n","        # -------------------------\n","        # Load Sentinel-2 (2 bands)\n","        # -------------------------\n","        with rasterio.open(s2_path) as src:\n","            s2_raw = src.read([1, 2])       # B4=RED, B8=NIR (since you exported only these)\n","\n","        # -------------------------\n","        # Resize MODIS to 32×32\n","        # -------------------------\n","        modis_b1 = cv2.resize(modis_raw[0], (32, 32))\n","        modis_b2 = cv2.resize(modis_raw[1], (32, 32))\n","        modis_resized = np.stack([modis_b1, modis_b2], axis=0)\n","\n","        # -------------------------\n","        # Resize Sentinel-2 to 128×128\n","        # -------------------------\n","        s2_b4 = cv2.resize(s2_raw[0], (128, 128))\n","        s2_b8 = cv2.resize(s2_raw[1], (128, 128))\n","        s2_resized = np.stack([s2_b4, s2_b8], axis=0)\n","\n","        # -------------------------\n","        # Normalize 0–1\n","        # -------------------------\n","        modis_resized = modis_resized / np.max(modis_resized)\n","        s2_resized = s2_resized / np.max(s2_resized)\n","\n","        # Convert to float tensors\n","        modis_tensor = torch.tensor(modis_resized, dtype=torch.float32)\n","        s2_tensor = torch.tensor(s2_resized, dtype=torch.float32)\n","\n","        return modis_tensor, s2_tensor\n","\n","# -----------------------------------------------------\n","# 3. CNN MODEL (MODIS → Sentinel HR)\n","# -----------------------------------------------------\n","class FusionModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(2, 32, 3, padding=1),   # 2 MODIS bands\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, 3, padding=1),\n","            nn.ReLU()\n","        )\n","\n","        self.up = nn.Sequential(\n","            nn.ConvTranspose2d(64, 32, 4, stride=4),  # upsample 32→128\n","            nn.ReLU(),\n","        )\n","\n","        self.decoder = nn.Sequential(\n","            nn.Conv2d(32, 16, 3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 2, 3, padding=1)  # 2 Sentinel bands (B4, B8)\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.up(x)\n","        x = self.decoder(x)\n","        return x\n","\n","# -----------------------------------------------------\n","# 4. LOAD DATA\n","# -----------------------------------------------------\n","dataset = FusionDataset(MODIS_DIR, S2_DIR, count=IMAGE_COUNT)\n","loader = DataLoader(dataset, batch_size=4, shuffle=True)\n","\n","# -----------------------------------------------------\n","# 5. TRAINING SETUP\n","# -----------------------------------------------------\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using device:\", device)\n","\n","model = FusionModel().to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# -----------------------------------------------------\n","# 6. TRAIN THE MODEL\n","# -----------------------------------------------------\n","EPOCHS = 50\n","\n","for epoch in range(EPOCHS):\n","    total_loss = 0\n","    for modis, s2 in loader:\n","        modis = modis.to(device)\n","        s2 = s2.to(device)\n","\n","        optimizer.zero_grad()\n","        output = model(modis)\n","        loss = criterion(output, s2)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss:.4f}\")\n","\n","# -----------------------------------------------------\n","# 7. SAVE MODEL\n","# -----------------------------------------------------\n","torch.save(model.state_dict(), \"fusion_model.pth\")\n","print(\"\\nModel Saved Successfully as fusion_model.pth!\")\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import rasterio\n","import numpy as np\n","import cv2\n","\n","# -------------------------------------------------------\n","# 1️⃣ MODEL DEFINITION (same as training)\n","# -------------------------------------------------------\n","class FusionModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(2, 32, 3, padding=1),  # 2 input channels: MODIS Red + NIR\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, 3, padding=1),\n","            nn.ReLU()\n","        )\n","        # Upsampling\n","        self.up = nn.Sequential(\n","            nn.ConvTranspose2d(64, 32, 4, stride=4),\n","            nn.ReLU(),\n","        )\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","            nn.Conv2d(32, 16, 3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 2, 3, padding=1)  # Output: 2 bands (Red + NIR)\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.up(x)\n","        x = self.decoder(x)\n","        return x\n","\n","# -------------------------------------------------------\n","# 2️⃣ LOAD TRAINED MODEL\n","# -------------------------------------------------------\n","model = FusionModel()\n","model.load_state_dict(torch.load(\"fusion_model.pth\", map_location=\"cpu\"))\n","model.eval()\n","print(\"Model loaded successfully!\")\n","\n","# -------------------------------------------------------\n","# 3️⃣ LOAD MODIS IMAGE (2 bands)\n","# -------------------------------------------------------\n","modis_path = \"MODIS_sample_10.tif\"\n","\n","with rasterio.open(modis_path) as src:\n","    modis_b1 = src.read(1)  # Red\n","    modis_b2 = src.read(2)  # NIR\n","    profile = src.profile  # reuse metadata for output\n","\n","print(\"MODIS bands loaded:\", modis_b1.shape, modis_b2.shape)\n","\n","# -------------------------------------------------------\n","# 4️⃣ PREPROCESS MODIS IMAGE\n","# -------------------------------------------------------\n","# Resize both bands to 32x32\n","modis_b1_resized = cv2.resize(modis_b1, (32, 32))\n","modis_b2_resized = cv2.resize(modis_b2, (32, 32))\n","\n","# Stack bands → [2, 32, 32]\n","modis_tensor = np.stack([modis_b1_resized, modis_b2_resized], axis=0)\n","\n","# Normalize to 0–1 (same as training)\n","modis_tensor = modis_tensor.astype(np.float32) / np.max(modis_tensor)\n","\n","# Convert to PyTorch tensor and add batch dimension → [1, 2, 32, 32]\n","inp = torch.tensor(modis_tensor).unsqueeze(0)\n","\n","# -------------------------------------------------------\n","# 5️⃣ PREDICT HIGH-RES SENTINEL-LIKE IMAGE\n","# -------------------------------------------------------\n","with torch.no_grad():\n","    pred = model(inp)\n","\n","pred_np = pred.squeeze().numpy()  # shape: [2, 128, 128]\n","red_pred = pred_np[0]\n","nir_pred = pred_np[1]\n","\n","print(\"Prediction done. Output shape:\", pred_np.shape)\n","\n","# -------------------------------------------------------\n","# 6️⃣ SAVE PREDICTED SENTINEL IMAGE\n","# -------------------------------------------------------\n","output_path = \"Predicted_S2_sample_10.tif\"\n","\n","new_profile = profile.copy()\n","new_profile.update({\n","    \"height\": 128,\n","    \"width\": 128,\n","    \"count\": 2,\n","    \"dtype\": \"float32\"\n","})\n","\n","with rasterio.open(output_path, \"w\", **new_profile) as dst:\n","    dst.write(pred_np)\n","\n","print(\"Saved fused Sentinel-2 image →\", output_path)\n","\n","# -------------------------------------------------------\n","# 7️⃣ COMPUTE NDVI\n","# -------------------------------------------------------\n","ndvi = (nir_pred - red_pred) / (nir_pred + red_pred + 1e-6)\n","\n","ndvi_path = \"Predicted_NDVI_10.tif\"\n","\n","new_profile.update({\"count\": 1})\n","\n","with rasterio.open(ndvi_path, \"w\", **new_profile) as dst:\n","    dst.write(ndvi.astype(np.float32), 1)\n","\n","print(\"Saved NDVI map →\", ndvi_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NIClDirm1DPm","executionInfo":{"status":"ok","timestamp":1765230219615,"user_tz":-300,"elapsed":52,"user":{"displayName":"Kishwer Basri","userId":"16853879575023419513"}},"outputId":"68a1053e-a4d3-42de-e7ff-aee0c9b4e91e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Model loaded successfully!\n","MODIS bands loaded: (45, 69) (45, 69)\n","Prediction done. Output shape: (2, 128, 128)\n","Saved fused Sentinel-2 image → Predicted_S2_sample_10.tif\n","Saved NDVI map → Predicted_NDVI_10.tif\n"]}]}]}